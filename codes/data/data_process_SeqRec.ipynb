{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def neg_sample(all_items, pos_items, n_sample=99):\n",
    "    random.seed(42)\n",
    "    pos_set = set(pos_items)  \n",
    "    neg_items = [item for item in all_items if item not in pos_set]  \n",
    "    return random.sample(neg_items, k=n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 497577/497577 [00:00<00:00, 1221352.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55223 17408 497577 0.0005175966099616421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/games/meta_Video_Games.json') as f:\n",
    "    metadata = [json.loads(line) for line in f]\n",
    "with open('../../datasets/games/Video_Games_5.json') as f:\n",
    "    reviews = [json.loads(line) for line in f]\n",
    "users = set()\n",
    "items = set()\n",
    "for review in tqdm(reviews):\n",
    "    users.add(review['reviewerID'])\n",
    "    items.add(review['asin'])\n",
    "item2id = dict()\n",
    "count = 0\n",
    "for item in items:\n",
    "    item2id[item] = count\n",
    "    count += 1\n",
    "print(len(users), len(items), len(reviews), len(reviews) / (len(users) * len(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84819/84819 [00:00<00:00, 1025397.89it/s]\n",
      "100%|██████████| 497577/497577 [00:01<00:00, 335235.99it/s]\n"
     ]
    }
   ],
   "source": [
    "id_title = {}\n",
    "id_item = {}\n",
    "cnt = 0\n",
    "for meta in tqdm(metadata):\n",
    "    if len(meta['title']) > 1: # remove the item without title\n",
    "        id_title[meta['asin']] = meta['title']\n",
    "users = dict()\n",
    "for review in tqdm(reviews):\n",
    "    user = review['reviewerID']\n",
    "    if 'asin' not in review:\n",
    "        break\n",
    "    item = review['asin']\n",
    "    if item not in id_title:\n",
    "        continue\n",
    "    if review['asin'] not in id_item:\n",
    "        id_item[review['asin']] = cnt\n",
    "        cnt += 1\n",
    "    if 'overall' not in review:\n",
    "        continue\n",
    "    if 'unixReviewTime' not in review:\n",
    "        continue\n",
    "    if user not in users:\n",
    "        users[user] = {\n",
    "            'items': [],\n",
    "            'ratings': [],\n",
    "            'timestamps': [],\n",
    "            'reviews': [],\n",
    "        }\n",
    "    users[user]['items'].append(item)\n",
    "    users[user]['ratings'].append(review['overall'])\n",
    "    users[user]['timestamps'].append(review['unixReviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../datasets/games/SeqRec/id2name.txt\") as f:\n",
    "    all_items = []\n",
    "    for line in f.readlines():\n",
    "        all_items.append(line.split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55223/55223 [00:03<00:00, 15970.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 0\n",
    "interactions = []\n",
    "B = []\n",
    "for key in tqdm(users.keys()):\n",
    "    items = users[key]['items']\n",
    "    ratings = users[key]['ratings']\n",
    "    timestamps = users[key]['timestamps']\n",
    "    all = list(zip(items, ratings, timestamps))\n",
    "    res = sorted(all, key=lambda x: int(x[-1]))\n",
    "    items, ratings, timestamps = zip(*res)\n",
    "    items, ratings, timestamps = list(items), list(ratings), list(timestamps)\n",
    "    users[key]['items'] = items\n",
    "    users[key]['item_ids'] = [item2id[x] for x in items]\n",
    "    users[key]['item_titles'] = [id_title[x] for x in items]\n",
    "    users[key]['ratings'] = ratings\n",
    "    users[key]['timestamps'] = timestamps\n",
    "    for i in range(min(10, len(items) - 1), len(items)):\n",
    "        st = max(i - 10, 0)\n",
    "        interactions.append([key, users[key]['items'][st: i], users[key]['items'][i], users[key]['item_ids'][st: i], users[key]['item_ids'][i], users[key]['item_titles'][st: i], users[key]['item_titles'][i], ratings[st: i], ratings[i], int(timestamps[i])])   \n",
    "print(len(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = sorted(interactions, key=lambda x: x[-1])\n",
    "import csv\n",
    "test_size = 5000\n",
    "valid_size = 5000\n",
    "train_size = len(interactions) - test_size - valid_size\n",
    "with open('../../datasets/games/SeqRec/train.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[:train_size])\n",
    "with open('../../datasets/games/SeqRec/valid.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[train_size:(train_size+valid_size)])\n",
    "with open('../../datasets/games/SeqRec/test.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[(train_size+valid_size):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(input_path, output_path, sample=False):\n",
    "    data = pd.read_csv(input_path)\n",
    "    if sample:\n",
    "        data = data.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "        data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
    "    json_list = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "    #    row[\"user id\"] = eval(row['user_id'])\n",
    "        row['history_item_title'] = eval(row['history_item_title'])\n",
    "        row['history_rating'] = eval(row['history_rating'])\n",
    "        L = len(row['history_item_title'])\n",
    "        history = \"The user has played the following video games before:\"\n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                history += \"\\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "            else:\n",
    "                history += \", \\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "        target_movie = str(row['item_title'])\n",
    "        target_movie_str = \"\\\"\" + target_movie + \"\\\"\"\n",
    "        json_list.append({\n",
    "            \"user id\" : row[\"user_id\"],\n",
    "            \"instruction\": \"Given a list of video games the user has played before, please recommend a new video game that the user likes to the user.\",\n",
    "            \"input\": f\"{history}\\n \",\n",
    "            \"output\": target_movie_str,\n",
    "        })        \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139796it [00:23, 5837.35it/s]\n",
      "5000it [00:00, 5124.16it/s]\n",
      "5000it [00:00, 5842.95it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"../../datasets/games/SeqRec\"\n",
    "csv_to_json(f'{path}/train.csv', f'{path}/train.json')\n",
    "csv_to_json(f'{path}/valid.csv', f'{path}/valid.json')\n",
    "csv_to_json(f'{path}/test.csv', f'{path}/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:04<00:00, 1034.67it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../datasets/games/SeqRec/test.json\", \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test_ = []\n",
    "random.seed(42)\n",
    "for t in tqdm(test):\n",
    "    t[\"neg_samples\"] = neg_sample(all_items, users[t[\"user id\"]][\"item_titles\"], n_sample=99)\n",
    "    test_.append(t)\n",
    "with open(\"../../datasets/games/SeqRec/test.json\", \"w\") as f:\n",
    "    json.dump(test_, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203766it [00:00, 321763.59it/s]\n",
      "100%|██████████| 203766/203766 [00:02<00:00, 100957.82it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []  \n",
    "with open('../../datasets/movies/meta_Movies_and_TV.json', 'r') as f:  \n",
    "    for line in tqdm(f):\n",
    "        results.append(line)\n",
    "import re\n",
    "new_datas = []\n",
    "for data in tqdm(results):\n",
    "    new_data = {}\n",
    "    asin = re.findall(r'\"asin\": \"(.*?)\"', data)\n",
    "    title = re.findall(r'\"title\": \"(.*?)\"' , data.replace(\"\\'\", \"_\"))\n",
    "    brand = re.findall(r'\"brand\": \"(.*?)\"' , data.replace(\"\\'\", \"_\"))\n",
    "    if len(brand) > 0:\n",
    "        brand = brand[0]\n",
    "    else:\n",
    "        brand = None\n",
    "    if len(asin) > 0:\n",
    "        new_data[\"asin\"] = asin[0]\n",
    "    if len(title) > 0:\n",
    "        new_data[\"title\"] = title[0]\n",
    "        if brand is not None:\n",
    "            new_data[\"title\"] += f\" - {brand}\"\n",
    "    new_datas.append(new_data)\n",
    "with open('../../datasets/movies/SeqRec/meta_movie_process.json', \"w\") as f:\n",
    "    json.dump(new_datas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3410019it [00:28, 117939.13it/s]\n",
      "100%|██████████| 3410019/3410019 [00:02<00:00, 1181681.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297529 60175 3410019 0.00019046334058915956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open('../../datasets/movies/SeqRec/meta_movie_process.json') as f:\n",
    "    metadata = json.load(f)\n",
    "reviews = []\n",
    "with open('../../datasets/movies/Movies_and_TV_5.json') as f:\n",
    "    for line in tqdm(f):\n",
    "        review = json.loads(line)\n",
    "        review = {\n",
    "            'reviewerID' : review['reviewerID'],\n",
    "            \"asin\" : review[\"asin\"],\n",
    "            \"overall\" : review[\"overall\"],\n",
    "            \"unixReviewTime\" : review[\"unixReviewTime\"]\n",
    "        }\n",
    "        reviews.append(review)\n",
    "    \n",
    "users = set()\n",
    "items = set()\n",
    "for review in tqdm(reviews):\n",
    "    users.add(review['reviewerID'])\n",
    "    items.add(review['asin'])\n",
    "item2id = dict()\n",
    "count = 0\n",
    "for item in items:\n",
    "    item2id[item] = count\n",
    "    count += 1\n",
    "print(len(users), len(items), len(reviews), len(reviews) / (len(users) * len(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203766/203766 [00:00<00:00, 1810502.06it/s]\n",
      "100%|██████████| 3410019/3410019 [00:05<00:00, 635319.76it/s] \n"
     ]
    }
   ],
   "source": [
    "id_title = {}\n",
    "id_item = {}\n",
    "cnt = 0\n",
    "for meta in tqdm(metadata):\n",
    "    if \"title\" in meta and len(meta['title']) > 50: # remove movies with titles that are too short to be distinguished\n",
    "        id_title[meta['asin']] = meta['title']\n",
    "\n",
    "users = dict()\n",
    "for review in tqdm(reviews):\n",
    "    user = review['reviewerID']\n",
    "    if 'asin' not in review:\n",
    "        break\n",
    "    item = review['asin']\n",
    "    if item not in id_title:\n",
    "        continue\n",
    "    if review['asin'] not in id_item:\n",
    "        id_item[review['asin']] = cnt\n",
    "        cnt += 1\n",
    "    if 'overall' not in review:\n",
    "        continue\n",
    "    if 'unixReviewTime' not in review:\n",
    "        continue\n",
    "    if user not in users:\n",
    "        users[user] = {\n",
    "            'items': [],\n",
    "            'ratings': [],\n",
    "            'timestamps': [],\n",
    "            'reviews': []\n",
    "        }\n",
    "    users[user]['items'].append(item)\n",
    "    users[user]['ratings'].append(review['overall'])\n",
    "    users[user]['timestamps'].append(review['unixReviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216367/216367 [00:05<00:00, 37205.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 0\n",
    "interactions = []\n",
    "B = []\n",
    "for key in tqdm(users.keys()):\n",
    "    items = users[key]['items']\n",
    "    ratings = users[key]['ratings']\n",
    "    timestamps = users[key]['timestamps']\n",
    "    all = list(zip(items, ratings, timestamps))\n",
    "    res = sorted(all, key=lambda x: int(x[-1]))\n",
    "    items, ratings, timestamps = zip(*res)\n",
    "    items, ratings, timestamps = list(items), list(ratings), list(timestamps)\n",
    "    users[key]['items'] = items\n",
    "    users[key]['item_ids'] = [item2id[x] for x in items]\n",
    "    users[key]['item_titles'] = [id_title[x] for x in items]\n",
    "    users[key]['ratings'] = ratings\n",
    "    users[key]['timestamps'] = timestamps\n",
    "    for i in range(min(10, len(items) - 1), len(items)):\n",
    "        st = max(i - 10, 0)\n",
    "        if i - st < 3:\n",
    "            continue\n",
    "        interactions.append([key, users[key]['items'][st: i], users[key]['items'][i], users[key]['item_ids'][st: i], users[key]['item_ids'][i], users[key]['item_titles'][st: i], users[key]['item_titles'][i], ratings[st: i], ratings[i], int(timestamps[i])])   \n",
    "print(len(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114594/114594 [00:00<00:00, 268371.49it/s]\n"
     ]
    }
   ],
   "source": [
    "id2name = {}\n",
    "for data in tqdm(interactions):\n",
    "    ids = data[3] + [data[4]]\n",
    "    names = data[5] + [data[6]]\n",
    "    for id, name in zip(ids, names):\n",
    "        if id not in id2name.keys():\n",
    "            id2name[id] = name\n",
    "        else:\n",
    "            if id2name[id] != name:\n",
    "                print(\"error\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name = dict(sorted(id2name.items(), key=lambda x: x[1]))\n",
    "all_items = list(id2name.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16233/16233 [00:00<00:00, 408190.27it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_id2name = {}\n",
    "processed_id2name_set = set([])\n",
    "for _, name in tqdm(id2name.items()):\n",
    "    if name not in processed_id2name_set:\n",
    "        processed_id2name[len(processed_id2name)] = name\n",
    "        processed_id2name_set.add(name)\n",
    "with open(\"../../datasets/movies/SeqRec/id2name.json\", \"w\") as f:\n",
    "    json.dump(processed_id2name, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = sorted(interactions, key=lambda x: x[-1])\n",
    "import csv\n",
    "test_size = 5000\n",
    "valid_size = 5000\n",
    "train_size = len(interactions) - test_size - valid_size\n",
    "with open('../../datasets/movies/SeqRec/train.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[:train_size])\n",
    "with open('../../datasets/movies/SeqRec/valid.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[train_size:(train_size+valid_size)])\n",
    "with open('../../datasets/movies/SeqRec/test.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[(train_size+valid_size):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(input_path, output_path, sample=False):\n",
    "    data = pd.read_csv(input_path)\n",
    "    \n",
    "    if sample:\n",
    "        data = data.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "        data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
    "    json_list = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        row['history_item_title'] = eval(row['history_item_title'])\n",
    "        row['history_rating'] = eval(row['history_rating'])\n",
    "        L = len(row['history_item_title'])\n",
    "        history = \"The user has watched the following movies and TVs before:\"  \n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                history += \"\\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "            else:\n",
    "                history += \", \\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "        target_movie = str(row['item_title'])\n",
    "        target_movie_str = \"\\\"\" + target_movie + \"\\\"\"\n",
    "        json_list.append({\n",
    "            \"user id\" : row[\"user_id\"],\n",
    "            \"instruction\": \"Given a list of movies ad TVs the user has watched before, please recommend the title of a new movie or TV that the user will like to watch in the following time.\",\n",
    "            \"input\": f\"{history}\\n \",\n",
    "            \"output\": target_movie_str,\n",
    "        })        \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104594it [00:15, 6589.42it/s]\n",
      "5000it [00:00, 6521.96it/s]\n",
      "5000it [00:00, 7021.61it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"../../datasets/movies/SeqRec\"\n",
    "csv_to_json(f'{path}/train.csv', f'{path}/train.json')\n",
    "csv_to_json(f'{path}/valid.csv', f'{path}/valid.json')\n",
    "csv_to_json(f'{path}/test.csv', f'{path}/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:05<00:00, 849.55it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../datasets/movies/SeqRec/test.json\", \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test_ = []\n",
    "random.seed(42)\n",
    "for t in tqdm(test):\n",
    "    t[\"neg_samples\"] = neg_sample(all_items, users[t[\"user id\"]][\"item_titles\"], n_sample=99)\n",
    "    test_.append(t)\n",
    "with open(\"../../datasets/movies/SeqRec/test.json\", \"w\") as f:\n",
    "    json.dump(test_, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "results = []  \n",
    "with open('../../datasets/food/meta_Grocery_and_Gourmet_Food.json', 'r') as f:  \n",
    "    for line in f:\n",
    "        results.append(line)\n",
    "import re\n",
    "new_datas = []\n",
    "for data in results:\n",
    "    new_data = {}\n",
    "    asin = re.findall(r\"_asin_:\\s*_(.*?)_\" , data.replace(\"\\'\", \"_\"))\n",
    "    title = re.findall(r\"_title_:\\s*_(.*?)_\" , data.replace(\"\\'\", \"_\"))\n",
    "    brand = re.findall(r\"_brand_:\\s*_(.*?)_\" , data.replace(\"\\'\", \"_\"))\n",
    "    if len(brand) > 0:\n",
    "        brand = brand[0]\n",
    "    else:\n",
    "        brand = None\n",
    "    if len(asin) > 0:\n",
    "        new_data[\"asin\"] = asin[0]\n",
    "    if len(title) > 0:\n",
    "        new_data[\"title\"] = title[0]\n",
    "        if brand is not None and len(brand)>0:\n",
    "            new_data[\"title\"] += f\" - {brand}\"\n",
    "    new_datas.append(new_data)\n",
    "with open('../../datasets/food/SeqRec/meta_food_process.json', \"w\") as f:\n",
    "    json.dump(new_datas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151254it [00:02, 58705.39it/s]\n",
      "100%|██████████| 151254/151254 [00:00<00:00, 1456889.07it/s]\n",
      "100%|██████████| 8713/8713 [00:00<00:00, 1585671.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14681 8713 151254 0.0011824519884614812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open('../../datasets/food/SeqRec/meta_food_process.json') as f:\n",
    "    metadata = json.load(f)\n",
    "with open('../../datasets/food/Grocery_and_Gourmet_Food_5.json') as f:\n",
    "    reviews = [json.loads(line) for line in tqdm(f)]\n",
    "users = set()\n",
    "items = set()\n",
    "for review in tqdm(reviews):\n",
    "    users.add(review['reviewerID'])\n",
    "    items.add(review['asin'])\n",
    "item2id = dict()\n",
    "count = 0\n",
    "for item in tqdm(items):\n",
    "    item2id[item] = count\n",
    "    count += 1\n",
    "print(len(users), len(items), len(reviews), len(reviews) / (len(users) * len(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171760/171760 [00:00<00:00, 1623503.98it/s]\n",
      "100%|██████████| 151254/151254 [00:00<00:00, 611163.92it/s]\n"
     ]
    }
   ],
   "source": [
    "id_title = {}\n",
    "id_item = {}\n",
    "cnt = 0\n",
    "for meta in tqdm(metadata):\n",
    "    if \"title\" in meta and len(meta['title']) > 1: # remove the item without title\n",
    "        id_title[meta['asin']] = meta['title']\n",
    "\n",
    "users = dict()\n",
    "for review in tqdm(reviews):\n",
    "    user = review['reviewerID']\n",
    "    if 'asin' not in review:\n",
    "        break\n",
    "    item = review['asin']\n",
    "    if item not in id_title:\n",
    "        continue\n",
    "    if review['asin'] not in id_item:\n",
    "        id_item[review['asin']] = cnt\n",
    "        cnt += 1\n",
    "    if 'overall' not in review:\n",
    "        continue\n",
    "    if 'unixReviewTime' not in review:\n",
    "        continue\n",
    "    if user not in users:\n",
    "        users[user] = {\n",
    "            'items': [],\n",
    "            'ratings': [],\n",
    "            'timestamps': [],\n",
    "            'reviews': []\n",
    "        }\n",
    "    users[user]['items'].append(item)\n",
    "    users[user]['ratings'].append(review['overall'])\n",
    "    users[user]['timestamps'].append(review['unixReviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14641/14641 [00:00<00:00, 45220.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 0\n",
    "interactions = []\n",
    "B = []\n",
    "for key in tqdm(users.keys()):\n",
    "    items = users[key]['items']\n",
    "    ratings = users[key]['ratings']\n",
    "    timestamps = users[key]['timestamps']\n",
    "    all = list(zip(items, ratings, timestamps))\n",
    "    res = sorted(all, key=lambda x: int(x[-1]))\n",
    "    items, ratings, timestamps = zip(*res)\n",
    "    items, ratings, timestamps = list(items), list(ratings), list(timestamps)\n",
    "    users[key]['items'] = items\n",
    "    users[key]['item_ids'] = [item2id[x] for x in items]\n",
    "    users[key]['item_titles'] = [id_title[x] for x in items]\n",
    "    users[key]['ratings'] = ratings\n",
    "    users[key]['timestamps'] = timestamps\n",
    "    for i in range(min(10, len(items) - 1), len(items)):\n",
    "        st = max(i - 10, 0)\n",
    "        interactions.append([key, users[key]['items'][st: i], users[key]['items'][i], users[key]['item_ids'][st: i], users[key]['item_ids'][i], users[key]['item_titles'][st: i], users[key]['item_titles'][i], ratings[st: i], ratings[i], int(timestamps[i])])   \n",
    "print(len(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43293/43293 [00:00<00:00, 284677.34it/s]\n"
     ]
    }
   ],
   "source": [
    "id2name = {}\n",
    "for data in tqdm(interactions):\n",
    "    ids = data[3] + [data[4]]\n",
    "    names = data[5] + [data[6]]\n",
    "    for id, name in zip(ids, names):\n",
    "        if id not in id2name.keys():\n",
    "            id2name[id] = name\n",
    "        else:\n",
    "            if id2name[id] != name:\n",
    "                print(\"error\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name = dict(sorted(id2name.items(), key=lambda x: x[1]))\n",
    "all_items = list(id2name.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7069/7069 [00:00<00:00, 13731.60it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_id2name = {}\n",
    "for _, name in tqdm(id2name.items()):\n",
    "    if name not in processed_id2name.values():\n",
    "        processed_id2name[len(processed_id2name)] = name\n",
    "with open(\"../../datasets/food/SeqRec/id2name.json\", \"w\") as f:\n",
    "    json.dump(processed_id2name, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = sorted(interactions, key=lambda x: x[-1])\n",
    "import csv\n",
    "test_size = round(len(interactions)*0.1)\n",
    "valid_size = round(len(interactions)*0.1)\n",
    "train_size = len(interactions) - test_size - valid_size\n",
    "with open('../../datasets/food/SeqRec/train.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[:train_size])\n",
    "with open('../../datasets/food/SeqRec/valid.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[train_size:(train_size+valid_size)])\n",
    "with open('../../datasets/food/SeqRec/test.csv', 'w') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['user_id', 'item_asins', 'item_asin', 'history_item_id', 'item_id', 'history_item_title', 'item_title', 'history_rating', 'rating', 'timestamp'])\n",
    "    csvwriter.writerows(interactions[(train_size+valid_size):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(input_path, output_path, sample=False):\n",
    "    data = pd.read_csv(input_path)\n",
    "    \n",
    "    if sample:\n",
    "        data = data.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "        data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
    "    json_list = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        row['history_item_title'] = eval(row['history_item_title'])\n",
    "        row['history_rating'] = eval(row['history_rating'])\n",
    "        L = len(row['history_item_title'])\n",
    "        history = \"The user has purchased the following food before:\"  \n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                history += \"\\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "            else:\n",
    "                history += \", \\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "        target_movie = str(row['item_title'])\n",
    "        target_movie_str = \"\\\"\" + target_movie + \"\\\"\"\n",
    "        json_list.append({\n",
    "            \"user id\" : row[\"user_id\"],\n",
    "            \"instruction\": \"Given a list of food the user has purchased before, please recommend the name of a new food that the user will like to purchase in the following time.\",\n",
    "            \"input\": f\"{history}\\n \",\n",
    "            \"output\": target_movie_str,\n",
    "        })        \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34635it [00:05, 5995.58it/s]\n",
      "4329it [00:01, 4140.50it/s]\n",
      "4329it [00:00, 7250.85it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"../../datasets/food/SeqRec\"\n",
    "csv_to_json(f'{path}/train.csv', f'{path}/train.json')\n",
    "csv_to_json(f'{path}/valid.csv', f'{path}/valid.json')\n",
    "csv_to_json(f'{path}/test.csv', f'{path}/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4329/4329 [00:02<00:00, 1660.50it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../datasets/food/SeqRec/test.json\", \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test_ = []\n",
    "random.seed(42)\n",
    "for t in tqdm(test):\n",
    "    t[\"neg_samples\"] = neg_sample(all_items, users[t[\"user id\"]][\"item_titles\"], n_sample=99)\n",
    "    test_.append(t)\n",
    "with open(\"../../datasets/food/SeqRec/test.json\", \"w\") as f:\n",
    "    json.dump(test_, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchmth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
